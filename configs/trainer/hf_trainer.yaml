bf16: true
bf16_full_eval: true
dataloader_drop_last: true
dataloader_num_workers: 4
dataloader_pin_memory: true
do_eval: true
do_train: true
evaluation_strategy: "steps"
eval_steps: 100
gradient_accumulation_steps: 1
gradient_checkpointing: false
learning_rate: 5e-4
log_on_each_node: false
logging_steps: 1
lr_scheduler_type: "cosine"
max_grad_norm: 1.0
num_train_epochs: 1
optim: "adamw_torch"
per_device_eval_batch_size: 512
per_device_train_batch_size: 512
save_steps: 5000
save_strategy: "steps"
save_total_limit: 20
torch_compile: false
warmup_ratio: 0.0
weight_decay: 1e-4