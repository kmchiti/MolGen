model:
  name: "GPT2LMHeadModel"
  config:
    vocab_size: 50
    n_positions: 512
    n_ctx: 512
    n_embd: 768
    n_layer: 12
    n_head: 12
    resid_pdrop: 0.1
    embd_pdrop: 0.1
    attn_pdrop: 0.1
    layer_norm_epsilon: 1e-5